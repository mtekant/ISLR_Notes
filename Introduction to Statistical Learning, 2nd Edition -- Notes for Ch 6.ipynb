{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Introduction to Statistical Learning, 2nd Edition</b>\n",
    "## by James, Witten, Hastie, Tibshirani\n",
    "\n",
    "### Notes by Melis Tekant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6  - Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are often quite competitive in real-world problems, and have the advantage of interpretability. These models can be improved by using fitting procedures other than least squares, such as subset selection, shrinkage, and dimension reduction.\n",
    "\n",
    "Subset selection:\n",
    "\n",
    "- Best subset selection: fit least squares regression for each combination of p predictors (leading to $2^p$ models) and identify the best. Do this by setting $M_0$ as the null model with no predictors, then for k = 1,2,...,p, fit all ${p \\choose k}$ models with k predictors, pick the best among them (one which has the lowest RSS), and label it $M_k$. (For logistic regression, instead of RSS, $\\textit{deviance} = -2ln(L)$ is used.) Then select the best $M_k$ using cross-validated prediction error, AIC, BIC, or adjusted R-squared. This way, the number of stored models reduces to p+1. This method is highly computationally intensive, so for large values of p, computational shortcuts (branch-and-bound techniques) can be used to eliminate some choices, or alternative methods that are more computationally efficient can be used. \n",
    "\n",
    "AIC ($\\propto C_p$): Akaike information criterion. It is a relative estimator, where the lower the value is, the better the model is. It is often used for time series data. \n",
    "AIP $ = -2ln(L) + 2k$, where the $ln(L)$ term denotes the log-likelihood, and $k$ is the number of parameters, acting as a penalty for the AIC value.\n",
    "\n",
    "(Note: this definition differs from the definition in the book: $C_p = \\frac{1}{n}(RSS + 2k \\hat \\sigma^2)$, yet are equivalent given that the residuals are normally distributed.  \n",
    "\n",
    "This can be shown via the following: Given the response $y = \\omega X + \\epsilon$, where $\\epsilon \\sim N(0,\\sigma)$, the maxmimum likelihood is calculated by \n",
    "\n",
    "$$ L = \\frac{1}{(2 \\pi)^{N/2} \\sigma^N} e^{-\\frac{\\sum(y-\\omega X)^2}{2\\sigma^2}}.$$\n",
    "\n",
    "Therefore, $ln(L) = A + (-\\frac{\\sum(y-\\omega X)^2}{2 \\sigma^2})$, where A is a constant involving N and $\\sigma$. \n",
    "\n",
    "Since $C_p$ is a relative estimator, it can be scaled, and constants can be added or removed from it, given that the process is done for each value that will be compared. \n",
    "\n",
    "Scaling the definition of $C_p$ in the text by $n/\\hat \\sigma^2$ gives equivalent results to the above definition with an additional constant and a scaling factor.)\n",
    "\n",
    "BIC: Bayesian information criterion. Similar to AIC, but a different penalty term for complexity.\n",
    "BIC $= -2ln(L) + ln(n)k$ $(\\propto \\frac{1}{n}(RSS+log(n)k\\hat\\sigma^2)$, where n is the number of data points. For n>7, this model places a larger penalty for k, so it typically selects for smaller number of predictors when compared to AIC. \n",
    "\n",
    "Adjusted R-squared: R-squared model that has been modified for the number of predictors, to penalize increasing k to prevent overfitting. \n",
    "\n",
    "$R^2 = 1- \\frac{RSS}{TSS}$, where TSS = $\\sum(y_i-\\bar y)^2$.\n",
    "\n",
    "Adjusted $R^2 = 1-\\frac{RSS/(n-k-1)}{TSS/(n-1)}$.\n",
    "\n",
    "Alternatively, test error can be estimated using validation set and cross-validation methods, which make fewer assumptions about the true model, and can be used even when $\\sigma$ or k is hard to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Own',\n",
       "       'Student', 'Married', 'Region', 'Balance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_csv('/Users/melistekant/Documents/Python Projects/ISLR2/Credit.csv')\n",
    "credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Own</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>West</th>\n",
       "      <th>South</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Own  Student  Married  \\\n",
       "0   14.891   3606     283      2   34         11    1        0        0   \n",
       "1  106.025   6645     483      3   82         15    1        1        1   \n",
       "2  104.593   7075     514      4   71         11    0        0        0   \n",
       "3  148.924   9504     681      3   36         11    0        0        1   \n",
       "4   55.882   4897     357      2   68         16    1        0        0   \n",
       "\n",
       "   Balance  West  South  \n",
       "0      333     0      1  \n",
       "1      903     1      0  \n",
       "2      580     1      0  \n",
       "3      964     1      0  \n",
       "4      331     0      1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate Region into 2 dummy variables\n",
    "\n",
    "dummy_south = np.zeros(len(credit.Region))\n",
    "dummy_west = np.zeros(len(credit.Region))\n",
    "dummy_south[credit.Region == 'South'] = 1 \n",
    "dummy_west[credit.Region == 'West'] = 1\n",
    "credit2 = credit.drop(columns = 'Region', axis =1)\n",
    "credit2['West'] = dummy_west.astype('int')\n",
    "credit2['South'] = dummy_south.astype('int')\n",
    "\n",
    "yndict = {'Yes':1, 'No': 0}\n",
    "credit2[['Own','Student','Married']] = credit2[{'Own','Student','Married'}].applymap(lambda x: yndict[x])\n",
    "credit2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = credit2.Balance\n",
    "X = credit2.drop(columns = 'Balance',axis=1)\n",
    "knum = np.arange(0,len(credit2.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['South', 'West', 'East'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.Region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stepwise selection: For large p where best subset selection might not be usable, stepwise methods, which test much more restricted models, can be used.\n",
    "    \n",
    "     Forward stepwise selection: Start with no predictors ($M_0$) and add one predictor at a time (the one that gives the greatest additional improvement to the fit, measured by lowest RSS or highest R-squared, call $M_k$ for k predictors in the model) until all are in the model. This leads to $1+p(p+1)/2$ fits instead of $2^p$. Select the best $M_k$ using cross-validated prediction error, AIC, BIC, or adjusted R-squared. \n",
    "     \n",
    "     Backward stepwise selection: Similar to forward stewpsie selection, but start with all predictors, and remove predictors one at a time, selecting the best k-1 predictors (given by lowest RSS or highest R-squared value) at each step, then find best $M_k$ by one of the metrics outlined above. This method requires n>p, while forward stepwise selection does not. \n",
    "     \n",
    "     Hybrid approaches: Predictors are added to the model, but can also be removed, if they do not provide improvement in the new fit. This approach tries to marry the subset selection model with the computational advantages of stepwise selection methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Table 6.1 goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the error vs. k graph is quite flat, use the 'one-standard-error rule': calculate standard error and estimate MSE for each k. Select the simplest (smallest k) model for which test error is within one standard error of the lowest error value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
